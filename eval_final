# app.py
import streamlit as st
from typing import List, Dict
import os
import time
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
# from langchain_core.runnables import RunnableSequence  # removed - unused
import json
import re
import hashlib
from datetime import datetime
# from pathlib import Path  # removed - unused
import logging
import csv
import io
import base64
import httpx
try:
    # We no longer use the `audio_recorder_streamlit` recorder component.
    # Keep the import guarded in case other modules expect it, but mark
    # recorder availability as False so recording UI is not shown.
    from audio_recorder_streamlit import audio_recorder  # pragma: no cover - optional
    AUDIO_AVAILABLE = False
except Exception:
    AUDIO_AVAILABLE = False

# -------------------------
# Configuration
# -------------------------
# Load environment variables
load_dotenv()

# Ensure a usable SSL_CERT_FILE is set early so httpx/openai won't try to
# read an administrator-only CA file. If the configured path is not readable
# we replace it with certifi's bundle when available.
try:
    import certifi as _certifi

    def _readable(path):
        try:
            if not path:
                return False
            with open(path, "rb"):
                return True
        except Exception:
            return False

    _cand = os.environ.get("SSL_CERT_FILE")
    if not _readable(_cand):
        # If we have generated a combined CA bundle in the project (e.g. certs/combined_ca.pem),
        # prefer that since it may include the private CA that signs the LLM host certificate.
        try:
            from pathlib import Path
            project_root = Path(__file__).resolve().parent
            combined_path = project_root / "certs" / "combined_ca.pem"
            if combined_path.exists() and _readable(str(combined_path)):
                os.environ["SSL_CERT_FILE"] = str(combined_path)
                logging.info(f"SSL_CERT_FILE set to project combined CA bundle: {combined_path}")
            else:
                os.environ["SSL_CERT_FILE"] = _certifi.where()
                logging.info("SSL_CERT_FILE set to certifi bundle to avoid unreadable system CA file")
        except Exception:
            os.environ["SSL_CERT_FILE"] = _certifi.where()
            logging.info("SSL_CERT_FILE set to certifi bundle to avoid unreadable system CA file")
except Exception:
    # If certifi isn't installed or anything goes wrong, leave environment as-is.
    pass

# Ensure Python/requests/ssl use a readable CA bundle inside the venv
# This prevents PermissionError when a system `SSL_CERT_FILE` points to
# an administrator-only file (e.g. C:\\Users\\Administrator\\cacert.pem).
try:
    import certifi
    os.environ.setdefault("SSL_CERT_FILE", certifi.where())
except Exception:
    # If certifi isn't available for some reason, continue ‚Äî the earlier
    # behavior (using system env vars) will still apply.
    pass

st.session_state.audio_answer = None
if "admin_logged_in" not in st.session_state:
    st.session_state.admin_logged_in = False
if "admin_username" not in st.session_state:
    st.session_state.admin_username = None

# Ensure DEEPSEEK_API_KEY is set in your environment
if "DEEPSEEK_API_KEY" not in os.environ:
    st.warning("Set environment variable DEEPSEEK_API_KEY before running. Example (PowerShell): $env:DEEPSEEK_API_KEY='sk-...'")
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")

# Also expose a shorter alias for other internal uses
API_KEY = DEEPSEEK_API_KEY

# --- Admin credentials (use env vars with safe fallback) ---
ADMIN_USERNAME = os.environ.get("ADMIN_USERNAME", "admin")
ADMIN_PASSWORD = os.environ.get("ADMIN_PASSWORD", "Admin@123")  # change in env for production


# Create an httpx client for LLM calls. Support a safe certifi fallback
# when `SSL_CERT_FILE` points to an unreadable system file, and also
# support an opt-in insecure mode controlled by `ALLOW_INSECURE_LLM` for
# development/testing only.
def _is_readable(path):
    try:
        if not path:
            return False
        with open(path, "rb"):
            return True
    except Exception:
        return False

# If explicitly allowed, disable TLS verification for the LLM client.
ALLOW_INSECURE = os.environ.get("ALLOW_INSECURE_LLM", "").lower() in ("1", "true", "yes", "on")
if ALLOW_INSECURE:
    logging.warning("ALLOW_INSECURE_LLM enabled: TLS verification DISABLED for LLM http client (INSECURE)")
    client = httpx.Client(verify=False)
else:
    # Prefer the env var if it's readable, otherwise override with certifi.
    _env_cert = os.environ.get("SSL_CERT_FILE")
    if not _is_readable(_env_cert):
        try:
            os.environ["SSL_CERT_FILE"] = certifi.where()
            logging.info("SSL_CERT_FILE overridden to certifi bundle to avoid unreadable system CA file")
        except Exception:
            # If certifi isn't available or something goes wrong, leave env as-is
            pass

    _verify_path = os.environ.get("SSL_CERT_FILE")
    try:
        if _verify_path:
            client = httpx.Client(verify=_verify_path)
        else:
            client = httpx.Client(verify=True)
    except PermissionError:
        logging.warning("PermissionError reading SSL_CERT_FILE; removing env var and falling back to system verification")
        try:
            os.environ.pop("SSL_CERT_FILE", None)
        except Exception:
            pass
        client = httpx.Client(verify=True)

# Initialize the ChatOpenAI wrapper using our httpx client
client = httpx.Client(verify=False)
llm = ChatOpenAI(
    base_url="https://genailab.tcs.in",
    model="azure_ai/genailab-maas-DeepSeek-V3-0324",
    api_key=API_KEY,
    http_client=client,
)

# Role -> suggested skills mapping used to populate the skills multiselect
ROLE_SKILLS = {
    "Java Developer": [
        "Core Java",
        "Spring / Spring Boot",
        "Concurrency / Multithreading",
        "Maven / Gradle",
        "REST / Web APIs",
        "JPA / SQL",
        "Testing (JUnit)",
    ],
    "Database Administrator": [
        "PostgreSQL / MySQL",
        "Backup & Recovery",
        "Replication / HA",
        "Performance Tuning",
        "Security",
        "Monitoring",
    ],
    "Frontend Developer": [
        "HTML / CSS / JS",
        "React / Angular / Vue",
        "State Management",
        "Accessibility",
        "Responsive Design",
        "Testing (Jest)",
    ],
    "DevOps Engineer": [
        "Docker",
        "Kubernetes",
        "CI/CD",
        "Terraform / IaC",
        "Monitoring / Logging",
        "Linux / Scripting",
    ],
    "Data Engineer": [
        "Python / Scala",
        "ETL / Data Pipelines",
        "Spark",
        "Airflow",
        "Data Modeling",
        "SQL",
    ],
    "Python Developer": [
        "Core Python",
        "Flask / Django",
        "Async IO",
        "Testing (pytest)",
        "APIs",
        "Data Structures",
    ],
}

EVAL_HISTORY_DB = "evaluation_history.json"


def get_time_remaining(start_time, max_seconds):
    """Calculate remaining time in seconds"""
    if start_time is None:
        return max_seconds
    elapsed = time.time() - start_time
    remaining = max_seconds - elapsed
    return max(0, remaining)


def format_time(seconds):
    """Format seconds as MM:SS"""
    mins = int(seconds // 60)
    secs = int(seconds % 60)
    return f"{mins:02d}:{secs:02d}"


USERS_DB = "users.json"


def load_users():
    """Load users from JSON database"""
    if os.path.exists(USERS_DB):
        with open(USERS_DB, 'r') as f:
            return json.load(f)
    return {}

def save_users(users):
    """Save users to JSON database"""
    with open(USERS_DB, 'w') as f:
        json.dump(users, f, indent=2)

def load_eval_history():
    """Load evaluation history from JSON database"""
    if os.path.exists(EVAL_HISTORY_DB):
        with open(EVAL_HISTORY_DB, 'r') as f:
            return json.load(f)
    return {}

def save_eval_history(history):
    """Save evaluation history to JSON database"""
    with open(EVAL_HISTORY_DB, 'w') as f:
        json.dump(history, f, indent=2)


def eval_history_to_csv_for_user(username: str) -> str:
    """Generate CSV string for a single user's evaluation history."""
    history = load_eval_history()
    evs = history.get(username, [])
    out = io.StringIO()
    writer = csv.writer(out)
    # Header
    writer.writerow(["username", "date", "role", "total_score", "max_score", "percentage", "time_taken", "question_index", "question", "answer", "score", "feedback"])
    for ev in evs:
        date = ev.get('date')
        role = ev.get('role')
        total = ev.get('score')
        mx = ev.get('max_score')
        pct = ev.get('percentage')
        tt = ev.get('time_taken')
        qa_list = ev.get('qa_history', [])
        if not qa_list:
            writer.writerow([username, date, role, total, mx, pct, tt, "", "", "", "", ""])
        else:
            for idx, qa in enumerate(qa_list, start=1):
                writer.writerow([username, date, role, total, mx, pct, tt, idx, qa.get('q',''), qa.get('a',''), qa.get('score',''), qa.get('feedback','')])
    return out.getvalue()


def eval_history_to_csv_all() -> str:
    """Generate CSV string for all users' evaluation history."""
    history = load_eval_history()
    out = io.StringIO()
    writer = csv.writer(out)
    writer.writerow(["username", "date", "role", "total_score", "max_score", "percentage", "time_taken", "question_index", "question", "answer", "score", "feedback"])
    for username, evs in history.items():
        for ev in evs:
            date = ev.get('date')
            role = ev.get('role')
            total = ev.get('score')
            mx = ev.get('max_score')
            pct = ev.get('percentage')
            tt = ev.get('time_taken')
            qa_list = ev.get('qa_history', [])
            if not qa_list:
                writer.writerow([username, date, role, total, mx, pct, tt, "", "", "", "", ""])
            else:
                for idx, qa in enumerate(qa_list, start=1):
                    writer.writerow([username, date, role, total, mx, pct, tt, idx, qa.get('q',''), qa.get('a',''), qa.get('score',''), qa.get('feedback','')])
    return out.getvalue()

def hash_password(password):
    """Hash password using SHA256"""
    return hashlib.sha256(password.encode()).hexdigest()

def text_to_speech(text):
    """Convert text to speech using browser's Web Speech API"""
    # Create JavaScript code to use browser's speech synthesis
    speech_js = f"""
    <script>
    function speak() {{
        var msg = new SpeechSynthesisUtterance();
        msg.text = `{text}`;
        msg.lang = 'en-US';
        msg.rate = 0.9;
        msg.pitch = 1;
        window.speechSynthesis.speak(msg);
    }}
    speak();
    </script>
    """
    return speech_js

def create_timer_html(start_time: float, max_seconds: int, key: str) -> str:
    """Return HTML+JS for a client-side countdown timer that updates every second.

    start_time: epoch seconds (float). If 0 or None, timer will show full time and count down from now.
    """
    # Ensure numeric values are passed into the JS
    st = float(start_time) if start_time else 0.0
    ms = int(max_seconds)
    safe_key = str(key)
    html = f"""
    <div id="timer_container_{safe_key}" style="text-align:center;padding:6px 8px;">
        <div id="timer_label_{safe_key}" style="font-size:12px;color:#666;margin-bottom:6px;">Question Timer</div>
        <div id="timer_value_{safe_key}" style="font-family:monospace; font-size:20px; font-weight:700;">--:--</div>
    </div>
    <script>
    (function() {{
        var start = {st};
        var maxSecs = {ms};
        var label = document.getElementById('timer_label_{safe_key}');
        var valueEl = document.getElementById('timer_value_{safe_key}');

        function fmt(s) {{
            var m = Math.floor(s/60); var sec = Math.floor(s%60);
            return (m<10?('0'+m):m) + ':' + (sec<10?('0'+sec):sec);
        }}

        var autoSubmitted = false;
        function tryAutoSubmit() {{
            try {{
                // Use parent document when available (component runs inside an iframe)
                var hostDoc = (window.parent && window.parent.document) ? window.parent.document : document;
                // Find likely answer textarea(s) - prefer visible ones
                var candidates = Array.from(hostDoc.querySelectorAll('textarea'));
                var target = null;
                for (var i=0;i<candidates.length;i++){{
                    var el = candidates[i];
                    try {{ if (el.offsetParent === null) continue; }} catch(e) {{ /* some nodes may not expose offsetParent cross-doc */ }}
                    var ph = (el.getAttribute('placeholder') || '').toLowerCase();
                    if (ph.includes('type your answer') || ph.includes('write your code') || (el.id && el.id.includes('answer'))) {{ target = el; break; }}
                }}
                if (!target && candidates.length>0) target = candidates[candidates.length-1];

                var val = target ? (target.value || '').trim() : '';
                // Find buttons - Streamlit renders form buttons as <button> elements; prefer visible ones
                var buttons = Array.from(hostDoc.querySelectorAll('button'));
                var submitBtn = null, skipBtn = null;
                buttons.forEach(function(b){{
                    try {{
                        if (b.offsetParent === null) return; // hidden
                        var txt = (b.innerText || b.value || '').toLowerCase();
                        if (!submitBtn && (txt.includes('submit answer') || txt.includes('submit'))) submitBtn = b;
                        if (!skipBtn && (txt.includes('skip') || txt.includes("don't know") || txt.includes('dont know') || txt.includes("skip / don't know"))) skipBtn = b;
                    }} catch(e){{}}
                }});

                if (val && submitBtn) {{
                    console.log('[autosubmit] Answer present - auto-clicking Submit');
                    // Ensure Streamlit sees the latest value on the host document
                    try {{ target.dispatchEvent(new Event('input', {{ bubbles: true }})); }} catch(e) {{ console.warn('dispatch input failed', e); }}
                    try {{ submitBtn.click(); }} catch(e) {{ console.warn('submit click failed', e); }}
                    autoSubmitted = true;
                }} else if (!val && skipBtn) {{
                    console.log('[autosubmit] No answer - auto-clicking Skip');
                    try {{ skipBtn.click(); }} catch(e) {{ console.warn('skip click failed', e); }}
                    autoSubmitted = true;
                }} else {{
                    console.log('[autosubmit] Could not find target textarea or buttons to auto-submit.');
                }}
            }} catch(e) {{ console.warn('auto-submit failed', e); }}
        }}

        function update() {{
            var now = Date.now()/1000;
            var elapsed = start && start>0 ? now - start : 0;
            var remaining = Math.max(0, Math.round(maxSecs - elapsed));
            var color = remaining>300 ? '#16a34a' : (remaining>60 ? '#d97706' : '#dc2626');
            var emoji = remaining>300 ? 'üü¢' : (remaining>60 ? 'üü°' : 'üî¥');
            try {{
                valueEl.textContent = fmt(remaining);
                valueEl.style.color = color;
                label.textContent = emoji + ' Question Timer';
                // When timer reaches zero, attempt auto-submit once
                if (remaining <= 0 && !autoSubmitted) {{
                    // Give a short delay to allow any pending user input events to settle
                    setTimeout(tryAutoSubmit, 250);
                }}
            }} catch(e) {{ console.warn('timer update failed', e); }}
        }}

        update();
        setInterval(update, 1000);
    }})();
    </script>
    """
    return html

def create_total_timer_html(start_time: float, max_seconds: int, key: str) -> str:
    """Client-side total countdown timer that attempts to click a hidden auto-end button when expired."""
    st_val = float(start_time) if start_time else 0.0
    ms = int(max_seconds)
    safe_key = str(key)
    html = f"""
    <div id="total_timer_container_{safe_key}" style="text-align:center;padding:6px 8px;">
        <div id="total_timer_label_{safe_key}" style="font-size:12px;color:#666;margin-bottom:6px;">Total Interview Time</div>
        <div id="total_timer_value_{safe_key}" style="font-family:monospace; font-size:18px; font-weight:700;">--:--</div>
    </div>
    <script>
    (function() {{
        var start = {st_val};
        var maxSecs = {ms};
        var label = document.getElementById('total_timer_label_{safe_key}');
        var valueEl = document.getElementById('total_timer_value_{safe_key}');

        function fmt(s) {{
            var m = Math.floor(s/60); var sec = Math.floor(s%60);
            return (m<10?('0'+m):m) + ':' + (sec<10?('0'+sec):sec);
        }}

        var done = false;
        function tryAutoEnd() {{
            try {{
                var hostDoc = (window.parent && window.parent.document) ? window.parent.document : document;
                // Try to find a dedicated auto-end button by text
                var buttons = Array.from(hostDoc.querySelectorAll('button'));
                var target = null;
                buttons.forEach(function(b) {{
                    try {{
                        var txt = (b.innerText || b.value || '').toLowerCase();
                        if (!target && (txt.includes('auto end') || txt.includes('end interview') || txt.includes('finish interview') || txt.includes('export complete evaluation report') )) {{
                            target = b;
                        }}
                    }} catch(e){{}}
                }});

                if (target) {{
                    console.log('[total-autotimer] clicking auto-end target');
                    try {{ target.click(); }} catch(e) {{ console.warn('auto-end click failed', e); }}
                    done = true;
                }} else {{
                    // If no dedicated button, try to trigger a general rerun by clicking any visible interactive button (best-effort)
                    for (var i=0;i<buttons.length;i++) {{
                        try {{ if (buttons[i].offsetParent !== null) {{ buttons[i].click(); done = true; break; }} }} catch(e){{}}
                    }}
                    if (!done) console.log('[total-autotimer] no button found to click for auto-end');
                }}
            }} catch(e) {{ console.warn('tryAutoEnd failed', e); }}
        }}

        function update() {{
            var now = Date.now()/1000;
            var elapsed = start && start>0 ? now - start : 0;
            var remaining = Math.max(0, Math.round(maxSecs - elapsed));
            try {{
                valueEl.textContent = fmt(remaining);
                var color = remaining>600 ? '#16a34a' : (remaining>120 ? '#d97706' : '#dc2626');
                valueEl.style.color = color;
                if (remaining <= 0 && !done) {{
                    // Slight delay so any final events settle
                    setTimeout(tryAutoEnd, 250);
                }}
            }} catch(e) {{ console.warn('total timer update failed', e); }}
        }}

        update();
        setInterval(update, 1000);
    }})();
    </script>
    """
    return html

def create_code_ide_html(language="python", key="code_editor"):
    """Create an HTML component with Ace Editor for code input"""
    # Map language names to Ace editor modes
    lang_mode_map = {
        "python": "python",
        "javascript": "javascript",
        "java": "java",
        "c": "c_cpp",
        "cpp": "c_cpp",
        "go": "golang",
        "ruby": "ruby"
    }
    ace_mode = lang_mode_map.get(language.lower(), "python")
    
    html_code = f"""
    <div id="editor_{key}" style="height: 400px; width: 100%; border: 1px solid #ccc; border-radius: 5px;"></div>
    <div style="margin-top: 10px;">
        <button onclick="saveCode_{key}()" style="
            background-color: #4CAF50;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            margin-right: 10px;
            font-weight: 600;
        ">Save to Answer</button>
        <button onclick="clearCode_{key}()" style="
            background-color: #f44336;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            font-weight: 600;
        ">Clear</button>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.12/ace.js"></script>
    <script>
        var editor_{key} = ace.edit("editor_{key}");
        editor_{key}.setTheme("ace/theme/monokai");
        editor_{key}.session.setMode("ace/mode/{ace_mode}");
        editor_{key}.setFontSize(14);
        editor_{key}.setOptions({{
            enableBasicAutocompletion: true,
            enableLiveAutocompletion: true,
            showPrintMargin: false
        }});
        
        function saveCode_{key}() {{
            var code = editor_{key}.getValue();
            var textarea = parent.document.querySelector('textarea[aria-label="Your answer"]');
            if (textarea) {{
                // Use React-friendly way to set the value
                var nativeInputValueSetter = Object.getOwnPropertyDescriptor(window.HTMLTextAreaElement.prototype, "value").set;
                nativeInputValueSetter.call(textarea, code);
                
                // Trigger input and change events
                var inputEvent = new Event('input', {{ bubbles: true}});
                var changeEvent = new Event('change', {{ bubbles: true}});
                textarea.dispatchEvent(inputEvent);
                textarea.dispatchEvent(changeEvent);
                
                alert('Code saved to answer!');
            }} else {{
                alert('Could not find answer textarea');
            }}
        }}
        
        function clearCode_{key}() {{
            editor_{key}.setValue("");
        }}
    </script>
    """
    return html_code

def create_audio_player(text, key):
    """Create an audio player button that reads text aloud"""
    # Escape special characters for JavaScript - use JSON encoding for safety
    import json
    safe_text = json.dumps(text)
    
    audio_html = f"""
    <div id="tts_container_{key}">
        <button onclick="speakText_{key}()" id="btn_{key}" style="
            background-color: #667eea;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 14px;
            margin: 5px 0;
            box-shadow: 0 2px 5px rgba(0,0,0,0.2);
            transition: all 0.3s;
            font-weight: 600;
        " onmouseover="this.style.backgroundColor='#5568d3'" onmouseout="this.style.backgroundColor='#667eea'">
            üîä Listen to Question
        </button>
    </div>
    <script type="text/javascript">
    (function() {{
        // Ensure speech synthesis is loaded
        if ('speechSynthesis' in window) {{
            window.speakText_{key} = function() {{
                // Cancel any ongoing speech
                window.speechSynthesis.cancel();
                
                // Small delay to ensure cancel completes
                setTimeout(function() {{
                    // Create new utterance
                    var msg = new SpeechSynthesisUtterance();
                    msg.text = {safe_text};
                    msg.lang = 'en-US';
                    msg.rate = 0.85;
                    msg.pitch = 1.0;
                    msg.volume = 1.0;
                    
                    // Visual feedback
                    var btn = document.getElementById('btn_{key}');
                    if (btn) {{
                        btn.innerHTML = 'üî¥ Speaking...';
                        btn.style.backgroundColor = '#e74c3c';
                    }}
                    
                    msg.onend = function() {{
                        if (btn) {{
                            btn.innerHTML = 'üîä Listen to Question';
                            btn.style.backgroundColor = '#667eea';
                        }}
                    }};
                    
                    msg.onerror = function(event) {{
                        console.error('Speech synthesis error:', event);
                        if (btn) {{
                            btn.innerHTML = 'üîä Listen to Question';
                            btn.style.backgroundColor = '#667eea';
                        }}
                        alert('Speech error: ' + event.error + '. Please check your browser settings.');
                    }};
                    
                    // Load voices if not loaded yet
                    var voices = window.speechSynthesis.getVoices();
                    if (voices.length > 0) {{
                        // Try to find English voice
                        var englishVoice = voices.find(v => v.lang.startsWith('en'));
                        if (englishVoice) {{
                            msg.voice = englishVoice;
                        }}
                    }}
                    
                    // Speak
                    window.speechSynthesis.speak(msg);
                    console.log('TTS started for:', msg.text.substring(0, 50) + '...');
                }}, 100);
            }};
        }} else {{
            console.error('Speech synthesis not supported in this browser');
            var btn = document.getElementById('btn_{key}');
            if (btn) {{
                btn.innerHTML = '‚ùå TTS Not Supported';
                btn.disabled = true;
                btn.style.backgroundColor = '#999';
            }}
        }}
    }})();
    </script>
    """
    return audio_html

def verify_password(stored_hash, password):
    """Verify password against stored hash"""
    return stored_hash == hash_password(password)

def save_evaluation_result(username, eval_data):
    """Save evaluation result to user's history"""
    history = load_eval_history()
    if username not in history:
        history[username] = []
    
    eval_record = {
        "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "role": eval_data.get("role"),
        "score": eval_data.get("score"),
        "max_score": eval_data.get("max_score"),
        "percentage": eval_data.get("percentage"),
        "time_taken": eval_data.get("time_taken"),
        "qa_history": eval_data.get("qa_history", [])
    }
    
    history[username].append(eval_record)
    save_eval_history(history)

def home_page():
    # Page styling (removed blue gradient background)
    st.markdown("""
        <style>
        /* Removed gradient background to keep a neutral, clean page */
        .main {
            background: none !important;
            background-color: transparent !important;
        }

        .stTabs [data-baseweb="tab-list"] {
            background-color: rgba(255, 255, 255, 0.9);
            border-radius: 10px;
            padding: 10px;
        }
        </style>
    """, unsafe_allow_html=True)
    
    # Title section with centered logo above the title
    st.markdown('<div class="title-box">', unsafe_allow_html=True)
    logo_path = os.path.join("assets", "logo.png")
    if os.path.exists(logo_path):
        # Read the file and embed as base64 so HTML centering works reliably
        try:
            with open(logo_path, 'rb') as _f:
                _b64 = base64.b64encode(_f.read()).decode('utf-8')
            img_html = (
                f"<div style='text-align:center; margin-bottom:12px;'>"
                f"<img src='data:image/png;base64,{_b64}' "
                f"style='display:block; margin-left:auto; margin-right:auto; width:220px; height:auto;' alt='logo'/>"
                f"</div>"
            )
            st.markdown(img_html, unsafe_allow_html=True)
        except Exception:
            # fallback to st.image if embed fails
            st.image(logo_path, width=220)
    else:
        # Keep some spacing when logo isn't present
        st.markdown("<div style='height:32px'></div>", unsafe_allow_html=True)

    st.markdown("<h1 style='text-align: center; color: #667eea;'> Automated Candidate Evaluation System (AI Powered)</h1>", unsafe_allow_html=True)
    st.markdown("<p style='text-align: center; font-size: 18px; color: #555;'>Intelligent technical assessment with real-time AI feedback</p>", unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)
    
    # Tabs for Login, Register and Admin
    tab1, tab2, tab3 = st.tabs(["üîê Login", "üìù Register", "üõ†Ô∏è Admin"])
    
    with tab1:
        st.subheader("Login to Your Account")
        with st.form("login_form"):
            username = st.text_input("Username", key="login_username")
            password = st.text_input("Password", type="password", key="login_password")
            submitted = st.form_submit_button("Login", use_container_width=True)
            
            if submitted:
                if not username.strip() or not password.strip():
                    st.error("Please provide both username and password.")
                else:
                    users = load_users()
                    if username in users and verify_password(users[username]["password"], password):
                        st.session_state.logged_in = True
                        st.session_state.username = username
                        st.session_state.candidate = {
                            "name": users[username]["name"],
                            "experience": users[username]["experience"],
                            "email": users[username].get("email", "")
                        }
                        st.success(f"Welcome back, {users[username]['name']}! üéâ")
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error("Invalid username or password. Please try again.")
            
                
    with tab2:
        st.subheader("Create New Account")
        with st.form("register_form"):
            new_username = st.text_input("Username", key="reg_username", help="Choose a unique username")
            new_name = st.text_input("Full Name", key="reg_name")
            new_email = st.text_input("Email (Optional)", key="reg_email")
            new_experience = st.selectbox("Experience Level", ["Fresher", "1-3 years", "3-6 years", "6+ years"], key="reg_exp")
            new_password = st.text_input("Password", type="password", key="reg_password")
            confirm_password = st.text_input("Confirm Password", type="password", key="reg_confirm")
            register_submitted = st.form_submit_button("Register", use_container_width=True)
            
            if register_submitted:
                if not all([new_username.strip(), new_name.strip(), new_password.strip()]):
                    st.error("Please fill in all required fields.")
                elif new_password != confirm_password:
                    st.error("Passwords do not match!")
                elif len(new_password) < 6:
                    st.error("Password must be at least 6 characters long.")
                else:
                    users = load_users()
                    if new_username in users:
                        st.error("Username already exists. Please choose a different one.")
                    else:
                        users[new_username] = {
                            "name": new_name.strip(),
                            "email": new_email.strip(),
                            "experience": new_experience,
                            "password": hash_password(new_password),
                            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            # evaluation control fields
                            "eval_chances": {},       # role -> allowed attempts (default 1)
                            "eval_taken_counts": {}   # role -> attempts used
                        }
                        save_users(users)
                        st.success(f"Account created successfully! Welcome, {new_name}! üéâ")
                        st.info("Please login using the Login tab.")
        with tab3:
            st.subheader("Admin Console")
            if not st.session_state.admin_logged_in:
                with st.form("admin_login_form"):
                    admin_user = st.text_input("Admin Username", key="admin_login_user")
                    admin_pass = st.text_input("Admin Password", type="password", key="admin_login_pass")
                    admin_submit = st.form_submit_button("Login as Admin")
                    if admin_submit:
                        if admin_user == ADMIN_USERNAME and admin_pass == ADMIN_PASSWORD:
                            st.session_state.admin_logged_in = True
                            st.session_state.admin_username = admin_user
                            st.success("Admin logged in successfully.")
                            st.rerun()
                        else:
                            st.error("Invalid admin credentials.")
            else:
                st.info(f"üëë Admin: {st.session_state.admin_username}")
                users = load_users()
                user_list = list(users.keys())
                if not user_list:
                    st.info("No users found.")
                else:
                    sel_user = st.selectbox("Select user to manage", user_list, key="admin_sel_user")
                    if sel_user:
                        u = users.get(sel_user, {})
                        st.markdown(f"**User:** {u.get('name')}  ‚Ä¢  **Username:** {sel_user}")
                        st.markdown(f"**Email:** {u.get('email','')}  ‚Ä¢  **Experience:** {u.get('experience','')}")
                        st.markdown("---")
                        # Quick search and filter
                        st.subheader("Search & Quick Filter")
                        search_q = st.text_input("Filter usernames (partial)", key="admin_search_q")
                        filtered = [x for x in user_list if search_q.lower() in x.lower()] if search_q else user_list
                        if search_q:
                            sel_user = st.selectbox("Filtered users", filtered, key="admin_sel_user_filtered")
                            u = users.get(sel_user, {})
                            st.markdown(f"**User:** {u.get('name')}  ‚Ä¢  **Username:** {sel_user}")
                            st.markdown(f"**Email:** {u.get('email','')}  ‚Ä¢  **Experience:** {u.get('experience','')}")
                            st.markdown("---")
                        # Show evaluation attempts and chances
                        taken = u.get('eval_taken_counts', {})
                        chances = u.get('eval_chances', {})
                        st.subheader("Evaluation Attempts")
                        roles = ["Java Developer", "Database Administrator", "Frontend Developer", "DevOps Engineer", "Data Engineer", "Python Developer"]
                        for r in roles:
                            used = taken.get(r, 0)
                            allowed = chances.get(r, 1)
                            st.write(f"- {r}: {used} / {allowed} attempts used")

                        st.markdown("---")
                        st.subheader("Manage Attempts")
                        # Global defaults manager
                        st.caption("Global settings apply to all users when set below")
                        gcol1, gcol2 = st.columns([2,1])
                        with gcol1:
                            global_role = st.selectbox("Global role to set default attempts", roles, key="admin_global_role")
                            global_default = st.number_input("Default allowed attempts for this role (applies to all users)", min_value=1, max_value=100, value=1, key="admin_global_default")
                            if st.button("Apply Global Default to All Users"):
                                for uname, uu in users.items():
                                    uu.setdefault('eval_chances', {})[global_role] = int(global_default)
                                    users[uname] = uu
                                save_users(users)
                                st.success(f"Applied default {global_default} attempts for role {global_role} to all users.")
                        with gcol2:
                            if st.button("Bulk Reset All Users' Attempts for Role"):
                                for uname, uu in users.items():
                                    uu.setdefault('eval_taken_counts', {})[global_role] = 0
                                    users[uname] = uu
                                save_users(users)
                                st.success(f"Reset attempts for role {global_role} for ALL users.")
                        col1, col2 = st.columns(2)
                        with col1:
                            m_role = st.selectbox("Role to modify", roles, key="admin_role_modify")
                            m_allowed = st.number_input("Allowed attempts", min_value=1, max_value=100, value=chances.get(m_role, 1), key="admin_allowed")
                            if st.button("Set Allowed Attempts"):
                                u.setdefault('eval_chances', {})[m_role] = int(m_allowed)
                                users[sel_user] = u
                                save_users(users)
                                st.success(f"Set allowed attempts for {sel_user} / {m_role} to {m_allowed}")
                        with col2:
                            if st.button("Reset Attempts for Role"):
                                u.setdefault('eval_taken_counts', {})[m_role] = 0
                                users[sel_user] = u
                                save_users(users)
                                st.success(f"Reset attempts for {sel_user} / {m_role}")

                        if st.button("Reset All Attempts for User"):
                            u['eval_taken_counts'] = {}
                            users[sel_user] = u
                            save_users(users)
                            st.success(f"Reset all attempts for {sel_user}")

                        st.markdown("---")
                        st.subheader("Evaluation History")
                        history = load_eval_history()
                        user_evals = history.get(sel_user, [])
                        if not user_evals:
                            st.info("No evaluation history for this user.")
                        else:
                            for idx, ev in enumerate(reversed(user_evals), 1):
                                with st.expander(f"Eval #{len(user_evals)-idx+1} - {ev['date']} - {ev['role']}"):
                                    st.metric("Score", f"{ev['score']}/{ev['max_score']}")
                                    st.markdown(f"**Percentage:** {ev['percentage']:.1f}%")
                                    st.markdown(f"**Time:** {format_time(ev.get('time_taken',0))}")
                                    st.markdown("---")
                                    st.subheader("Question-wise")
                                    for q_idx, qa in enumerate(ev.get('qa_history', []), 1):
                                        st.markdown(f"<div class='no-copy'><strong>Q{q_idx}:</strong> {qa['q']}</div>", unsafe_allow_html=True)
                                        st.markdown(f"**Score:** {qa['score']}/20")
                                        st.markdown(f"**Feedback:** {qa['feedback']}")
                                        st.markdown("---")

                        # Export and download controls
                        st.markdown("---")
                        st.subheader("Export Data")
                        col_e1, col_e2 = st.columns(2)
                        with col_e1:
                            csv_user = eval_history_to_csv_for_user(sel_user)
                            if csv_user:
                                st.download_button(label="üì• Export This User's Evaluations (CSV)", data=csv_user, file_name=f"evaluations_{sel_user}.csv", mime="text/csv")
                            else:
                                st.info("No CSV available for this user.")
                        with col_e2:
                            csv_all = eval_history_to_csv_all()
                            if csv_all:
                                st.download_button(label="üì• Export ALL Evaluations (CSV)", data=csv_all, file_name="evaluations_all_users.csv", mime="text/csv")
                            else:
                                st.info("No evaluation history available to export.")
                        if st.button("Logout Admin"):
                            # Log out admin and redirect to Home
                            st.session_state.admin_logged_in = False
                            st.session_state.admin_username = None
                            st.session_state.page_redirect = "Home"
                            st.rerun()
    
    # Features section
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("### üéØ Smart Evaluation")
        st.write("AI-powered question generation tailored to your role and skills")
    with col2:
        st.markdown("### ‚è±Ô∏è Timed Assessment")
        st.write("10 minutes per question with real-time timer tracking")
    with col3:
        st.markdown("### üìä Instant Feedback")
        st.write("Get detailed scores and improvement suggestions immediately")

    # FAQ section for login (helpful hints for candidates)
    st.markdown("---")
    st.markdown(
        """
        <div id="faq_container" style="display:flex; justify-content:center;">
          <div style="max-width:820px; width:100%;">
            <h3 style="text-align:center; color:#333; margin-bottom:8px;">Frequently Asked Questions</h3>
            <details style="border:1px solid #eee; padding:10px; border-radius:8px; margin-bottom:8px;">
              <summary style="font-weight:700; cursor:pointer;">Q: I forgot my password. What should I do?</summary>
              <div style="padding-top:8px;">A: Contact an administrator to reset your password or create a new account.</div>
            </details>
            <details style="border:1px solid #eee; padding:10px; border-radius:8px; margin-bottom:8px;">
              <summary style="font-weight:700; cursor:pointer;">Q: How do I start an evaluation after logging in?</summary>
              <div style="padding-top:8px;">A: Go to <strong>New Evaluation</strong> from the sidebar, complete the setup, and click <strong>Start Evaluation</strong>.</div>
            </details>
            <details style="border:1px solid #eee; padding:10px; border-radius:8px; margin-bottom:8px;">
              <summary style="font-weight:700; cursor:pointer;">Q: Which browsers are supported?</summary>
              <div style="padding-top:8px;">A: Use the latest Chrome or Edge for best compatibility (required for TTS and the embedded code editor).</div>
            </details>
            <details style="border:1px solid #eee; padding:10px; border-radius:8px; margin-bottom:8px;">
              <summary style="font-weight:700; cursor:pointer;">Q: How many attempts do I have for an evaluation?</summary>
              <div style="padding-top:8px;">A: Attempts are configured per role; check the Admin Console or contact an admin to view/reset attempts.</div>
            </details>
            <details style="border:1px solid #eee; padding:10px; border-radius:8px; margin-bottom:8px;">
              <summary style="font-weight:700; cursor:pointer;">Q: Who do I contact for help?</summary>
              <div style="padding-top:8px;">A: Reach out to the configured admin user (<code>ADMIN_USERNAME</code>) or your organization's support contact.</div>
            </details>
          </div>
        </div>
        <script>
        (function() {
            try {
                var c = document.getElementById('faq_container');
                if (!c) return;
                var details = c.querySelectorAll('details');
                details.forEach(function(d) {
                    d.addEventListener('toggle', function(e) {
                        if (d.open) {
                            details.forEach(function(o) { if (o !== d) o.open = false; });
                        }
                    });
                });
            } catch (err) { console.warn('FAQ toggle script error', err); }
        })();
        </script>
        """,
        unsafe_allow_html=True,
    )

def build_question_prompt(role: str, skills: List[str], language: str, is_coding: bool = False, asked_questions: List[str] = None):
    # Template that asks the LLM to produce role-specific technical questions
    previous_questions = ""
    if asked_questions:
        # Sanitize any braces in previously asked questions so they do not
        # accidentally become template placeholders w   hen embedded into
        # the PromptTemplate. This avoids pydantic validation errors like
        # "Invalid variable name '...': Variable names cannot contain '.' or other chars".
        joined = ', '.join(asked_questions)
        joined_safe = joined.replace('{', '{{').replace('}', '}}')
        previous_questions = f"\n\nIMPORTANT: Do NOT repeat these previously asked questions: {joined_safe}\nGenerate a completely DIFFERENT question."
    
    if is_coding:
        template = (
            "You are an interview generator for the role of {role}. "
            "The candidate's listed skills: {skills}. "
            "Generate one UNIQUE coding problem or algorithm question that requires writing actual code. "
            "The question should ask the candidate to write a function, method, or code snippet. "
            "Make it practical and relevant to the role. "
            "Do not include answer or explanation. Output ONLY the question text. "
            f"Respond in {{language}}.{previous_questions}"
        )
    else:
        template = (
            "You are an interview generator for the role of {role}. "
            "The candidate's listed skills: {skills}. "
            "Generate one UNIQUE theoretical or conceptual technical question (not a behavioral question) that tests these skills. "
            "Focus on concepts, design patterns, best practices, or architecture. "
            "Do not include answer or explanation. Output ONLY the question text. "
            f"Respond in {{language}}.{previous_questions}"
        )
    
    prompt = PromptTemplate(
        template=template,
        input_variables=["role", "skills", "language"]
    )
    return prompt

def build_evaluator_prompt(role: str, skill_focus: str, question: str, candidate_answer: str, language: str, is_coding: bool = False):
    # Prompt the LLM to evaluate candidate answer and give numeric score 0-20 & short feedback.
    if is_coding:
        template = (
            "You are a strict technical interviewer/evaluator for the role of {role}. "
            "Skill focus: {skill_focus}. "
            "Question: {question}\n\n"
            "Candidate answer: {candidate_answer}\n\n"
            "This is a CODING question. Evaluate the code based on:\n"
            "1. Correctness - Does the code solve the problem? (8 points)\n"
            "2. Code Quality - Is it readable, well-structured? (4 points)\n"
            "3. Efficiency - Is the approach optimal? (4 points)\n"
            "4. Edge cases - Are they handled? (4 points)\n\n"
            "In {language}, provide a numeric score between 0 and 20 (20 = perfect). "
            "Include a reason explaining the score and 1-2 specific suggestions to improve the code. "
            "If no code is provided or answer is off-topic, give 0 points. "
            "Output in JSON with keys: score, reason, suggestions. "
        )
    else:
        template = (
            "You are a strict technical interviewer/evaluator for the role of {role}. "
            "Skill focus: {skill_focus}. "
            "Question: {question}\n\n"
            "Candidate answer: {candidate_answer}\n\n"
            "In {language}, first evaluate whether the answer addresses the technical requirements. "
            "Then give a numeric score between 0 and 20 (20 = perfect), followed by a one-sentence reason and 1-2 suggestions to improve. "
            "If the answer is off-topic, say it is off-topic and give 0 points. "
            "Output in JSON with keys: score, reason, suggestions. "
        )
    
    prompt = PromptTemplate(
        template=template,
        input_variables=["role", "skill_focus", "question", "candidate_answer", "language"]
    )
    return prompt

def gen_question(role: str, skills: List[str], language: str, question_num: int = 1, asked_questions: List[str] = None) -> tuple:
    # Questions 3 and 5 will be coding questions
    is_coding = question_num in [3, 5]
    prompt = build_question_prompt(role, ", ".join(skills), language, is_coding, asked_questions)
    
    # Use temperature > 0 for variety in questions
    varied_llm = ChatOpenAI(
        base_url="https://genailab.tcs.in",
        model="azure_ai/genailab-maas-DeepSeek-V3-0324",
        api_key=API_KEY,
        http_client=client,
        temperature=0.7,  # Add randomness to avoid repetition
        max_tokens=400
    )
    chain = prompt | varied_llm | StrOutputParser()
    question = chain.invoke({"role": role, "skills": ", ".join(skills), "language": language})
    return question.strip().strip('"'), is_coding

def evaluate_answer(role: str, skill_focus: str, question: str, answer: str, language: str, is_coding: bool = False) -> Dict:
    prompt = build_evaluator_prompt(role, skill_focus, question, answer, language, is_coding)
    chain = prompt | llm | StrOutputParser()
    res = chain.invoke({"role": role, "skill_focus": skill_focus, "question": question, "candidate_answer": answer, "language": language})
    # Try to parse simple "score: X" or JSON-like output; we'll be permissive
    # Expecting a JSON-like, but if not, we fallback to parsing digits.
    try:
        # attempt to find JSON substring
        jtext = res[res.find("{"):res.rfind("}")+1]
        parsed = json.loads(jtext)
        parsed['raw'] = res
        return parsed
    except Exception:
        # fallback: try to extract first integer 0-20
        m = re.search(r"(\b[0-1]?\d|20)\b", res)
        score = int(m.group(0)) if m else 0
        return {"score": score, "reason": res[:200], "suggestions": "", "raw": res}

# -------------------------
# UI
# -------------------------
def init_state():
    if "logged_in" not in st.session_state:
        st.session_state.logged_in = False
    if "candidate" not in st.session_state:
        st.session_state.candidate = {}
    if "role" not in st.session_state:
        st.session_state.role = None
    if "lang" not in st.session_state:
        st.session_state.lang = "English"
    if "skills" not in st.session_state:
        st.session_state.skills = []
    if "asked_questions" not in st.session_state:
        st.session_state.asked_questions = []
    if "qa_history" not in st.session_state:
        st.session_state.qa_history = []  # list of dicts: {q, a, score, feedback}
    if "question_count" not in st.session_state:
        st.session_state.question_count = 0
    if "finalized" not in st.session_state:
        st.session_state.finalized = False
    if "current_question" not in st.session_state:
        st.session_state.current_question = ""
    if "welcome_shown" not in st.session_state:
        st.session_state.welcome_shown = False
    if "last_ai_message" not in st.session_state:
        st.session_state.last_ai_message = ""
    if "current_is_coding" not in st.session_state:
        st.session_state.current_is_coding = False
    if "question_start_time" not in st.session_state:
        st.session_state.question_start_time = None
    if "total_start_time" not in st.session_state:
        st.session_state.total_start_time = None
    if "time_expired" not in st.session_state:
        st.session_state.time_expired = False
    if "username" not in st.session_state:
        st.session_state.username = None
    if "page_redirect" not in st.session_state:
        st.session_state.page_redirect = None
    if "voice_mode" not in st.session_state:
        st.session_state.voice_mode = False
    if "audio_answer" not in st.session_state:
        st.session_state.audio_answer = None

st.set_page_config(
    page_title="AI Candidate Evaluation System", 
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)
init_state()

# Sidebar navigation
if st.session_state.logged_in:
    st.sidebar.success(f"üë§ Logged in as: **{st.session_state.candidate.get('name')}**")
    menu = ["Home", "New Evaluation", "Evaluation History", "Results"]
    if st.sidebar.button("üö™ Logout"):
        # Save current evaluation if exists
        if st.session_state.finalized and st.session_state.qa_history:
            total_score = sum([q["score"] for q in st.session_state.qa_history])
            max_score = 20 * len(st.session_state.qa_history)
            time_taken = time.time() - st.session_state.total_start_time if st.session_state.total_start_time else 0
            save_evaluation_result(st.session_state.username, {
                "role": st.session_state.role,
                "score": total_score,
                "max_score": max_score,
                "percentage": (total_score / max_score * 100) if max_score else 0,
                "time_taken": time_taken,
                "qa_history": st.session_state.qa_history
            })
        
        # Clear session
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        # After clearing session, request redirect to Home so next render shows the Home page
        st.session_state.page_redirect = "Home"
        st.rerun()
else:
    menu = ["Home"]

# If a page redirect was requested (by a button), use it to set the sidebar selection
# We set the actual `sidebar_nav` session key when a redirect is present so the
# selectbox will immediately reflect the requested page on rerun instead of
# relying on the `index` parameter alone (which is ignored if the widget key
# already exists in session state).
_default_index = 0
if st.session_state.get("page_redirect") in menu:
    _target = st.session_state.get("page_redirect")
    try:
        _default_index = menu.index(_target)
    except Exception:
        _default_index = 0
    # Set the sidebar selection key so the widget uses this value on the next render
    try:
        st.session_state.sidebar_nav = _target
    except Exception:
        # If for some reason we cannot write the session key, fallback to index
        pass
    # clear the redirect after applying it
    st.session_state.page_redirect = None

choice = st.sidebar.selectbox("üìã Navigation", menu, index=_default_index, key="sidebar_nav")

if choice == "Home":
    if not st.session_state.logged_in:
        home_page()
    else:
        st.title("üè† Dashboard")
        st.write(f"Welcome back, **{st.session_state.candidate.get('name')}**!")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.info("üìù Start a new evaluation from the **New Evaluation** page")
        with col2:
            history = load_eval_history()
            user_evals = history.get(st.session_state.username, [])
            st.metric("üéØ Total Evaluations", len(user_evals))
        with col3:
            if user_evals:
                avg_score = sum([e["percentage"] for e in user_evals]) / len(user_evals)
                st.metric("üìä Average Score", f"{avg_score:.1f}%")
        
        st.markdown("---")
        st.subheader("Quick Actions")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üöÄ Start New Evaluation", use_container_width=True):
                st.session_state.page_redirect = "New Evaluation"
                st.rerun()
        with col2:
            if st.button("üìú View Evaluation History", use_container_width=True):
                st.session_state.page_redirect = "Evaluation History"
                st.rerun()

elif choice == "New Evaluation":
    if not st.session_state.logged_in:
        st.info("Please login first from the Home page.")
        st.stop()

    st.header("Candidate Evaluation")
    st.markdown(f"**Candidate:** {st.session_state.candidate.get('name')}  ‚Ä¢  **Experience:** {st.session_state.candidate.get('experience')}")
    # Use an on_change callback so role changes take effect immediately.
    # Callbacks are not allowed inside `st.form`, so render these widgets
    # outside the form and keep the rest of the setup fields inside it.
    def _on_setup_role_change():
        new_role = st.session_state.get("setup_role")
        st.session_state.setup_selected_skills = []
        st.session_state.setup_other_skills = ""
        st.session_state.setup_last_role = new_role
        st.session_state.setup_role_changed = True
        # Keep the sidebar selection anchored on the New Evaluation page so
        # changing the role doesn't accidentally navigate away due to any
        # redirect/session-key races. Writing `sidebar_nav` here is safe and
        # ensures the user stays on the setup screen after the widget change.
        try:
            st.session_state.sidebar_nav = "New Evaluation"
            st.session_state.page_redirect = None
        except Exception:
            pass

    role = st.selectbox(
        "Choose role",
        ["Java Developer", "Database Administrator", "Frontend Developer", "DevOps Engineer", "Data Engineer", "Python Developer"],
        key="setup_role",
        on_change=_on_setup_role_change,
    )

    language = st.selectbox("Preferred language / Multilingual", ["English", "Hindi", "Spanish", "German", "French"], key="setup_lang")

    with st.form("setup_form"):
        # Show a role-specific multiselect of skills; allow optional free-text additions
        suggested = ROLE_SKILLS.get(role, [])

        # If role was changed via the callback, show a one-time info message
        if st.session_state.get("setup_role_changed"):
            st.info("Skills reset to match the newly selected role.")
            st.session_state.setup_role_changed = False

        # Use explicit keys so selections persist correctly across reruns.
        selected_skills = st.multiselect(
            "Select relevant skills (pick one or more)",
            options=suggested,
            key="setup_selected_skills",
            help="Pick the skills to focus the evaluation on",
        )

        other_skills = st.text_input(
            "Other skills (comma-separated)",
            key="setup_other_skills",
            help="Optional: add skills not listed above, comma-separated",
        )

        start = st.form_submit_button("Start Evaluation")
        if start:
            st.session_state.role = role
            st.session_state.lang = language
            # Merge selected and other skills into a single list
            skills = []
            if selected_skills:
                skills.extend([s for s in selected_skills if s and s.strip()])
            if other_skills:
                skills.extend([s.strip() for s in other_skills.split(",") if s.strip()])
            if not skills:
                st.warning("Please select or enter at least one skill to focus the interview.")
            else:
                    # Check user's remaining chances for this role
                    users = load_users()
                    user = users.get(st.session_state.username, {}) if st.session_state.username else {}
                    taken = user.get("eval_taken_counts", {}).get(role, 0)
                    allowed = user.get("eval_chances", {}).get(role, 1)
                    if taken >= allowed and not st.session_state.admin_logged_in:
                        st.error(f"You have used all allowed evaluation attempts for the role '{role}'. Contact an admin to reset attempts.")
                    else:
                        st.session_state.skills = skills
                        st.session_state.asked_questions = []
                        st.session_state.qa_history = []
                        st.session_state.question_count = 0
                        st.session_state.finalized = False
                        st.session_state.welcome_shown = False  # Reset welcome for new evaluation
                        st.session_state.current_question = ""
                        st.session_state.question_start_time = None
                        st.session_state.total_start_time = time.time()  # Start total timer
                        st.session_state.time_expired = False
                        st.success("Setup complete. Scroll down to the chat below.")
                        st.rerun()

    # Chat / interview UI
    if st.session_state.role and st.session_state.skills:
        st.subheader("Interview Chat")
        
        # Welcome message from AI (once)
        if not st.session_state.welcome_shown:
            welcome_text = f"Welcome {st.session_state.candidate.get('name')}! üëã\n\n"
            welcome_text += f"I'm your AI interviewer for the {st.session_state.role} position. "
            welcome_text += f"We will conduct a technical evaluation with 5 questions focusing on your skills: {', '.join(st.session_state.skills)}.\n\n"
            welcome_text += "üìã **Interview Format:**\n"
            welcome_text += "- Questions 1-2: Theoretical/Conceptual\n"
            welcome_text += "- Question 3: Coding Problem üíª\n"
            welcome_text += "- Question 4: Theoretical/Conceptual\n"
            welcome_text += "- Question 5: Coding Problem üíª\n\n"
            welcome_text += "Please answer each question to the best of your ability. You'll receive a score and feedback after each answer. Let's begin!"
            
            st.session_state.welcome_shown = True
            st.session_state.last_ai_message = welcome_text
            
            # Generate first question
            first_q, is_coding = gen_question(st.session_state.role, st.session_state.skills, st.session_state.lang, question_num=1, asked_questions=[])
            st.session_state.asked_questions.append(first_q)
            st.session_state.question_count = 1
            st.session_state.current_question = first_q
            st.session_state.current_is_coding = is_coding
            st.session_state.question_start_time = time.time()  # Start timer for first question
        
        # Check total time (50 minutes max)
        total_time_remaining = get_time_remaining(st.session_state.total_start_time, 50 * 60)
        if total_time_remaining <= 0 and not st.session_state.finalized:
            st.session_state.finalized = True
            st.session_state.time_expired = True
            st.error("‚è∞ Time's up! Total interview time (50 minutes) has expired.")
            st.info("üëâ Please go to the **Results** page to see your evaluation.")
            if st.button("View Results", key="view_results_time_expired"):
                st.session_state.page_redirect = "Results"
                st.rerun()
            st.stop()
        
        # Display total timer with auto-refresh
        col1, col2 = st.columns([3, 1])
        with col2:
            if not st.session_state.finalized:
                # Use client-side HTML timer component that auto-updates every second
                try:
                    st.components.v1.html(
                        create_total_timer_html(
                            st.session_state.total_start_time or time.time(),
                            50 * 60,  # 50 minutes
                            "total"
                        ),
                        height=80,
                        scrolling=False
                    )
                except Exception as e:
                    # Fallback to static display
                    st.metric("Total Time Left", format_time(total_time_remaining))
                # Auto-refresh the page while an evaluation is active so timers visibly count down
                # This is a lightweight development-friendly approach: reloads the page every few seconds
                # while the evaluation is running. Controlled by env var `TIMER_AUTO_RELOAD_SECONDS`.
                try:
                    _interval = int(os.environ.get("TIMER_AUTO_RELOAD_SECONDS", "0"))
                    if _interval > 0:
                        st.markdown(f"""
                        <script>
                        (function() {{
                            if (!window.__eval_timer_autoreload) {{
                                window.__eval_timer_autoreload = true;
                                var interval = {_interval} * 1000;
                                setInterval(function() {{
                                    try {{ if (document.visibilityState === 'visible') {{ window.location.reload(); }} }} catch(e) {{ window.location.reload(); }}
                                }}, interval);
                            }}
                        }})();
                        </script>
                        """, unsafe_allow_html=True)
                except Exception:
                    pass
        
        # Display welcome message
        if st.session_state.last_ai_message and not st.session_state.qa_history:
            st.info(st.session_state.last_ai_message)
        
        # Show previous QA history first (older questions at top)
        if st.session_state.qa_history:
            st.markdown("### üìù Interview Progress:")
            for idx, qa in enumerate(st.session_state.qa_history, start=1):
                # Determine if this was a coding question (questions 3 and 5)
                q_type_icon = "üíª" if idx in [3, 5] else "üí≠"
                with st.expander(f"{q_type_icon} Question {idx} - Score: {qa.get('score')}/20", expanded=False):
                    st.markdown(f"<div class='no-copy'><strong>Q:</strong> {qa['q']}</div>", unsafe_allow_html=True)
                    st.markdown(f"**Your Answer:** {qa['a']}")
                    st.markdown(f"**Feedback:** {qa.get('feedback')}")
            st.markdown("---")
        
        # Voice mode toggle
        st.markdown("---")
        col_v1, col_v2, col_v3 = st.columns([2, 1, 1])
        with col_v2:
            voice_enabled = st.toggle("üé§ Voice Mode", value=st.session_state.voice_mode, help="Enable voice input/output")
            if voice_enabled != st.session_state.voice_mode:
                st.session_state.voice_mode = voice_enabled
                st.rerun()
        
        
        # Display current question (newest at bottom)
        if st.session_state.current_question and st.session_state.question_count > 0 and st.session_state.question_count <= 5:
            # Check question time (10 minutes max per question)
            question_time_remaining = get_time_remaining(st.session_state.question_start_time, 10 * 60)
            
            question_type = "üíª Coding Question" if st.session_state.current_is_coding else "üí≠ Conceptual Question"
            st.markdown(f"### ‚ùì Question {st.session_state.question_count}: {question_type}")
            # Display question timer
            col1, col2, col3 = st.columns([3, 1, 0.5])
            with col1:
                # Render question inside a protected container to prevent copying
                st.markdown(
                    f"<div id='current_question' class='no-copy' style='font-size:16px;'><strong>{st.session_state.current_question}</strong></div>",
                    unsafe_allow_html=True,
                )

                # Inject JS/CSS to block copy/right-click for elements with class 'no-copy'
                if st.session_state.question_count and not st.session_state.finalized:
                    st.markdown(
                        """
                        <style>
                        .no-copy { user-select: none; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; }
                        </style>
                        <script>
                        (function(){
                          function blockEvent(e,msg){ e.preventDefault(); alert(msg); return false; }
                          var msgCopy = 'Copying questions is disabled during the evaluation to ensure integrity.';
                          var msgRight = 'Right-click is disabled during evaluation.';

                          document.addEventListener('copy', function(e){
                            var sel = window.getSelection();
                            if (!sel||sel.toString().trim()==='') return;
                            var node = sel.anchorNode;
                            while(node){
                              if (node.nodeType===1 && node.classList && node.classList.contains('no-copy')){ blockEvent(e,msgCopy); return; }
                              node = node.parentNode;
                            }
                          }, true);

                          document.addEventListener('cut', function(e){
                            var sel = window.getSelection();
                            var node = sel.anchorNode;
                            while(node){
                              if (node.nodeType===1 && node.classList && node.classList.contains('no-copy')){ blockEvent(e,msgCopy); return; }
                              node = node.parentNode;
                            }
                          }, true);

                          document.addEventListener('contextmenu', function(e){
                            var el = e.target;
                            while(el){ if (el.classList && el.classList.contains('no-copy')){ blockEvent(e,msgRight); return; } el = el.parentNode; }
                          }, true);

                          document.addEventListener('keydown', function(e){
                            if ((e.ctrlKey||e.metaKey) && e.key.toLowerCase()==='c'){
                              var sel = window.getSelection(); var node = sel.anchorNode;
                              while(node){ if (node.nodeType===1 && node.classList && node.classList.contains('no-copy')){ blockEvent(e,msgCopy); return; } node = node.parentNode; }
                            }
                          }, true);
                        })();
                        </script>
                        """,
                        unsafe_allow_html=True,
                    )

                # Add text-to-speech button for the question
                    if st.session_state.voice_mode:
                        # Use the components-based TTS player (rendered below) ‚Äî
                        # removed the inline HTML-based TTS button to avoid duplicates.
                        st.caption("üí° Click the 'Listen' control below to hear the question read aloud")
                        
            # Question timer column (render outside the voice-mode subcolumns so it's always visible)
            with col2:
                # Use client-side HTML timer component that auto-updates every second
                try:
                    st.components.v1.html(
                        create_timer_html(
                            st.session_state.question_start_time or time.time(),
                            3 * 60,  # 3 minutes per question
                            f"q{st.session_state.question_count}"
                        ),
                        height=60,
                        scrolling=False
                    )
                except Exception as e:
                    # Fallback to static display
                    timer_color = "üü¢" if question_time_remaining > 300 else "üü°" if question_time_remaining > 60 else "üî¥"
                    st.metric(f"{timer_color} Question Timer", format_time(question_time_remaining))
            with col3:
                if st.button("üîÑ", help="Refresh timer"):
                    st.rerun()
            # Auto-refresh question timer while evaluation is active.
            # Reloads the page every `TIMER_AUTO_RELOAD_SECONDS` seconds to update timers,
            # but avoid reloading while the candidate is typing in a textarea or when the
            # page is hidden (to prevent interrupting input).
            try:
                _q_interval = int(os.environ.get("TIMER_AUTO_RELOAD_SECONDS", "0"))
                if _q_interval > 0 and not st.session_state.finalized:
                    js = f"""
                    <script>
                    (function() {{
                        var intervalMs = {_q_interval} * 1000;
                        try {{ if (window.__st_question_timer) clearInterval(window.__st_question_timer); }} catch(e){{}}
                        window.__st_question_timer = setInterval(function() {{
                            try {{
                                // If user is typing in a textarea or an editable element, skip reload
                                var active = document.activeElement;
                                if (active && (active.tagName === 'TEXTAREA' || active.isContentEditable)) return;
                                // If the page is hidden (background tab), skip reload
                                if (document.hidden) return;
                                // Otherwise reload to update timers and metrics
                                window.location.reload();
                            }} catch(e) {{ /* swallow errors */ }}
                        }}, intervalMs);
                    }})();
                    </script>
                    """
                    st.markdown(js, unsafe_allow_html=True)
            except Exception:
                pass

            # Client-side countdown script: update the displayed total/question timers
            try:
                _total_start = int(st.session_state.total_start_time) if st.session_state.total_start_time else 0
                _question_start = int(st.session_state.question_start_time) if st.session_state.question_start_time else 0
                _total_max = 50 * 60
                _question_max = 10 * 60
                countdown_js = f"""
                <script>
                (function() {{
                    function pad(n) {{ return n < 10 ? '0' + n : '' + n; }}
                    function fmt(sec) {{ sec = Math.max(0, Math.floor(sec)); var m = Math.floor(sec/60); var s = sec%60; return pad(m)+':' + pad(s); }}
                    var totalStart = {_total_start};
                    var questionStart = {_question_start};
                    var totalMax = {_total_max};
                    var questionMax = {_question_max};

                    function tryClickSubmit() {{
                        try {{
                            var btns = document.querySelectorAll('button');
                            for (var i = 0; i < btns.length; i++) {{
                                var txt = (btns[i].innerText || '').trim().toLowerCase();
                                if (txt.startsWith('submit answer')) {{ btns[i].click(); return true; }}
                            }}
                            // fallback: click any button with 'submit' in text
                            for (var i = 0; i < btns.length; i++) {{
                                var txt = (btns[i].innerText || '').trim().toLowerCase();
                                if (txt.indexOf('submit') !== -1) {{ btns[i].click(); return true; }}
                            }}
                        }} catch(e){{}}
                        return false;
                    }}

                    function updateTimers() {{
                        var now = Math.floor(Date.now()/1000);
                        // total
                        if (totalStart && document.getElementById('total_timer_val')) {{
                            var rem = totalMax - (now - totalStart);
                            document.getElementById('total_timer_val').textContent = fmt(rem);
                        }}
                        // question
                        if (questionStart && document.getElementById('question_timer_val')) {{
                            var qrem = questionMax - (now - questionStart);
                            document.getElementById('question_timer_val').textContent = fmt(qrem);
                            // visual hint when low
                            var el = document.getElementById('question_timer_val');
                            if (qrem <= 60) {{ el.style.color = '#e74c3c'; }} else if (qrem <= 300) {{ el.style.color = '#f39c12'; }} else {{ el.style.color = '#2ecc71'; }}
                            // Auto-submit once when question time expires
                            if (qrem <= 0 && !window.__st_auto_submitted) {{
                                window.__st_auto_submitted = true;
                                // small delay to allow UI to update before clicking
                                setTimeout(function() {{ tryClickSubmit(); }}, 250);
                            }}
                        }}
                    }}
                    // Update immediately and every second
                    updateTimers();
                    if (window.__st_countdown_interval) clearInterval(window.__st_countdown_interval);
                    window.__st_countdown_interval = setInterval(function() {{
                        try {{ updateTimers(); }} catch(e){{}}
                    }}, 1000);
                }})();
                </script>
                """
                st.markdown(countdown_js, unsafe_allow_html=True)
            except Exception:
                pass

        # Candidate answer input
        if not st.session_state.finalized:
            # Check if question time expired
            question_time_remaining = get_time_remaining(st.session_state.question_start_time, 10 * 60)
            
                # Voice controls: only provide TTS to read the current question.
            if st.session_state.voice_mode:
                st.markdown("#### üé§ Voice Playback")
                st.info("Click the button to have the question read aloud by your browser.")
                # Render the TTS player using components.html to ensure scripts execute reliably
                try:
                    from streamlit.components import v1 as components
                    tts_html = create_audio_player(st.session_state.current_question, f"q{st.session_state.question_count}")
                    components.html(tts_html, height=120, scrolling=False)
                except Exception:
                    # Fallback to markdown embedding if components is unavailable
                    st.markdown(create_audio_player(st.session_state.current_question, f"q{st.session_state.question_count}"), unsafe_allow_html=True)
                
                st.markdown("---")
            
            with st.form("answer_form", clear_on_submit=True):
                # Pre-fill with audio transcription if available
                default_text = st.session_state.audio_answer if st.session_state.audio_answer else ""
                placeholder_text = "Write your code here..." if st.session_state.current_is_coding else "Type your answer here (or use voice input above)..."
                
                # Add code IDE for coding questions
                if st.session_state.current_is_coding:
                    st.markdown("### üíª Code Editor")
                    # Language selector for IDE
                    code_lang = st.selectbox(
                        "Select Programming Language",
                        ["python", "javascript", "java", "c", "cpp", "go", "ruby"],
                        index=0,
                        key="code_language_select"
                    )
                    # Render the IDE
                    st.components.v1.html(
                        create_code_ide_html(code_lang, key="coding_ide"),
                        height=500,
                        scrolling=True
                    )
                    st.markdown("**OR** type/paste your answer below:")
                
                answer = st.text_area("Your answer", value=default_text, key="answer_area", height=200 if st.session_state.current_is_coding else 150, placeholder=placeholder_text)
                
                col1, col2 = st.columns([1, 1])
                with col1:
                    submit_ans = st.form_submit_button("Submit Answer", type="primary")
                with col2:
                    skip_ans = st.form_submit_button("Skip / Don't Know")
                
                # Handle timeout
                if question_time_remaining <= 0:
                    st.warning("‚è∞ Time's up for this question! Auto-submitting...")
                    answer = answer if answer.strip() else "[No answer provided - Time expired]"
                    submit_ans = True
                
                if submit_ans or skip_ans:
                    if skip_ans:
                        answer = "[Skipped - Don't know]"
                    elif not answer.strip():
                        st.error("Please provide an answer before submitting.")
                        st.stop()
                    
                    # Evaluate answer
                    q = st.session_state.current_question
                    skill_focus = st.session_state.skills[0]  # for simplicity, focus on first skill (could be rotated)
                    is_coding = st.session_state.current_is_coding
                    
                    spinner_text = "ü§î Evaluating your code..." if is_coding else "ü§î Evaluating your answer..."
                    with st.spinner(spinner_text):
                        eval_result = evaluate_answer(st.session_state.role, skill_focus, q, answer, st.session_state.lang, is_coding)
                    
                    # Parse evaluation results
                    score = int(eval_result.get("score", 0))
                    reason = eval_result.get("reason", "") or eval_result.get("raw", "")
                    suggestions = eval_result.get("suggestions", "")
                    
                    # Save to history
                    st.session_state.qa_history.append({
                        "q": q, "a": answer, "score": score, "feedback": f"{reason} {suggestions}"
                    })
                    
                    # Display immediate feedback
                    st.markdown("---")
                    st.markdown("### üìä Evaluation Result")
                    
                    # Score display with color coding
                    if score >= 16:
                        st.success(f"**Score: {score}/20** - Excellent! ‚úÖ")
                    elif score >= 12:
                        st.info(f"**Score: {score}/20** - Good! üëç")
                    elif score >= 8:
                        st.warning(f"**Score: {score}/20** - Fair ‚ö†Ô∏è")
                    else:
                        st.error(f"**Score: {score}/20** - Needs Improvement üìö")
                    
                    # Detailed feedback
                    st.markdown(f"**üí¨ Feedback:** {reason}")
                    if suggestions:
                        st.markdown(f"**üí° Suggestions:** {suggestions}")
                    
                    st.markdown("---")
                    
                    # Check if interview is complete or move to next question
                    if st.session_state.question_count >= 5:
                        st.session_state.finalized = True
                        
                        # Save evaluation to history
                        total_score = sum([q["score"] for q in st.session_state.qa_history])
                        max_score = 20 * len(st.session_state.qa_history)
                        time_taken = time.time() - st.session_state.total_start_time if st.session_state.total_start_time else 0
                        save_evaluation_result(st.session_state.username, {
                            "role": st.session_state.role,
                            "score": total_score,
                            "max_score": max_score,
                            "percentage": (total_score / max_score * 100) if max_score else 0,
                            "time_taken": time_taken,
                            "qa_history": st.session_state.qa_history
                        })
                        # Increment user's taken count for this role
                        try:
                            users = load_users()
                            if st.session_state.username in users:
                                u = users[st.session_state.username]
                                taken = u.get("eval_taken_counts", {})
                                taken[st.session_state.role] = taken.get(st.session_state.role, 0) + 1
                                u["eval_taken_counts"] = taken
                                users[st.session_state.username] = u
                                save_users(users)
                        except Exception:
                            pass
                        
                        st.balloons()
                        st.success("üéâ Interview complete! You've answered all 5 questions.")
                        st.info("üëâ Please go to the **Results** page to see your final evaluation.")
                        # Mark that we should show the external 'View Results' button (rendered outside the form)
                        st.session_state.show_view_results = True
                    else:
                        # Generate next question
                        next_q_num = st.session_state.question_count + 1
                        with st.spinner("Preparing next question..."):
                            tries = 0
                            new_q = ""
                            new_is_coding = False
                            while tries < 8:
                                tries += 1
                                candidate_q, new_is_coding = gen_question(
                                    st.session_state.role, 
                                    st.session_state.skills, 
                                    st.session_state.lang, 
                                    question_num=next_q_num,
                                    asked_questions=st.session_state.asked_questions
                                )
                                # Check if question is truly unique (not just different wording)
                                is_duplicate = False
                                for asked in st.session_state.asked_questions:
                                    # Simple similarity check - if more than 50% of words match, consider it duplicate
                                    asked_words = set(asked.lower().split())
                                    candidate_words = set(candidate_q.lower().split())
                                    if len(asked_words) > 0:
                                        overlap = len(asked_words.intersection(candidate_words)) / len(asked_words)
                                        if overlap > 0.5:
                                            is_duplicate = True
                                            break
                                
                                if not is_duplicate:
                                    new_q = candidate_q
                                    break
                            
                            if not new_q:
                                # Fallback question if generation fails
                                if next_q_num in [3, 5]:
                                    new_q = f"Write a function to solve a common problem using {st.session_state.skills[0]}."
                                    new_is_coding = True
                                else:
                                    new_q = f"Explain an advanced concept or best practice related to {st.session_state.skills[0]}."
                                    new_is_coding = False
                            
                            st.session_state.asked_questions.append(new_q)
                            st.session_state.current_question = new_q
                            st.session_state.current_is_coding = new_is_coding
                            st.session_state.question_count = next_q_num
                            st.session_state.question_start_time = time.time()  # Reset timer for new question
                            st.session_state.audio_answer = None  # Clear previous audio recording
                        
                        st.success(f"‚úÖ Moving to Question {st.session_state.question_count}...")
                        time.sleep(1.5)  # Brief pause before refresh
                        st.rerun()

elif choice == "Evaluation History":
    if not st.session_state.logged_in:
        st.info("Please login first from the Home page.")
        st.stop()
    
    st.title("üìú Evaluation History")
    st.markdown(f"**User:** {st.session_state.candidate.get('name')}")
    
    history = load_eval_history()
    user_evals = history.get(st.session_state.username, [])
    
    if not user_evals:
        st.info("No evaluation history found. Complete your first evaluation to see results here!")
        if st.button("üöÄ Start New Evaluation"):
            st.session_state.page_redirect = "New Evaluation"
            st.rerun()
    else:
        st.success(f"You have completed **{len(user_evals)}** evaluation(s)")
        
        # Display evaluations in reverse chronological order (newest first)
        for idx, eval_data in enumerate(reversed(user_evals), 1):
            with st.expander(f"üìù Evaluation #{len(user_evals) - idx + 1} - {eval_data['date']} - {eval_data['role']}", expanded=(idx==1)):
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Role", eval_data['role'])
                with col2:
                    st.metric("Score", f"{eval_data['score']}/{eval_data['max_score']}")
                with col3:
                    st.metric("Percentage", f"{eval_data['percentage']:.1f}%")
                with col4:
                    st.metric("Time", format_time(eval_data['time_taken']))
                
                st.markdown("---")
                st.subheader("Question-wise Performance:")
                
                for q_idx, qa in enumerate(eval_data['qa_history'], 1):
                    q_type_icon = "üíª" if q_idx in [3, 5] else "üí≠"
                    with st.container():
                        st.markdown(f"**{q_type_icon} Question {q_idx}:** {qa['q']}")
                        st.markdown(f"**Your Answer:** {qa['a']}")
                        score_color = "üü¢" if qa['score'] >= 16 else "üü°" if qa['score'] >= 12 else "üü†" if qa['score'] >= 8 else "üî¥"
                        st.markdown(f"**{score_color} Score:** {qa['score']}/20")
                        st.markdown(f"**Feedback:** {qa['feedback']}")
                        st.markdown("---")

elif choice == "Results":
    if not st.session_state.logged_in:
        st.info("Please login first on the Login page.")
        st.stop()
    st.header("Interview Results")
    total_score = sum([q["score"] for q in st.session_state.qa_history])
    max_score = 20 * max(1, len(st.session_state.qa_history))
    pct = (total_score / max_score) * 100 if max_score else 0
    
    # Calculate time taken
    time_taken = 0
    if st.session_state.total_start_time:
        time_taken = time.time() - st.session_state.total_start_time
    
    st.markdown(f"**Candidate:** {st.session_state.candidate.get('name')}")
    st.markdown(f"**Role:** {st.session_state.role}  ‚Ä¢  **Language:** {st.session_state.lang}")
    
    # Display time info
    col1, col2 = st.columns(2)
    with col1:
        st.metric("üìä Total Score", f"{total_score} / {max_score} ({pct:.1f}%)")
    with col2:
        st.metric("‚è±Ô∏è Time Taken", format_time(time_taken))
    
    if st.session_state.time_expired:
        st.warning("‚ö†Ô∏è Note: Interview was terminated due to timeout (50 minutes exceeded).")
    
    st.markdown("---")
    st.markdown("### üìã Detailed Question-wise Performance")
    for i, qa in enumerate(st.session_state.qa_history, start=1):
        q_type_icon = "üíª" if i in [3, 5] else "üí≠"
        score_color = "üü¢" if qa['score'] >= 16 else "üü°" if qa['score'] >= 12 else "üü†" if qa['score'] >= 8 else "üî¥"
        with st.expander(f"{q_type_icon} Question {i} - {score_color} Score: {qa['score']}/20", expanded=False):
            st.markdown(f"<div class='no-copy'><strong>Q:</strong> {qa['q']}</div>", unsafe_allow_html=True)
            st.markdown(f"**Your Answer:** {qa['a']}")
            st.markdown(f"**Feedback:** {qa['feedback']}")

    st.markdown("---")
    
    # Final Recommendation Section
    st.markdown("## üéØ Final Recommendation")
    
    # Check if evaluation is complete (all 5 questions answered)
    if len(st.session_state.qa_history) < 5:
        st.info("‚è≥ **Evaluation in Progress**")
        st.write(f"Completed: {len(st.session_state.qa_history)}/5 questions")
        st.write("Please complete all questions to receive your final recommendation.")
    else:
        # Generate AI-powered recommendation
        with st.spinner("ü§ñ Generating detailed recommendation..."):
            # Prepare summary for AI
            qa_summary = ""
            for i, qa in enumerate(st.session_state.qa_history, 1):
                qa_summary += f"Q{i} (Score: {qa['score']}/20): {qa['q'][:100]}... Answer quality: {qa['feedback'][:150]}...\n"
            
            recommendation_prompt = PromptTemplate(
                template=(
                    "You are a senior technical hiring manager evaluating a candidate for {role} position.\n\n"
                    "Candidate Performance Summary:\n"
                    "- Total Score: {total_score}/{max_score} ({percentage:.1f}%)\n"
                    "- Questions Answered: 5 (2 conceptual, 2 coding, 1 conceptual)\n"
                    "- Time Taken: {time_taken} minutes\n\n"
                    "Question-wise breakdown:\n{qa_summary}\n\n"
                    "Based on this performance, provide:\n"
                    "1. Clear recommendation: RECOMMENDED or NOT RECOMMENDED\n"
                    "2. 2-3 key strengths observed\n"
                    "3. 2-3 areas for improvement\n"
                    "4. Brief rationale (2-3 sentences) explaining your recommendation\n\n"
                    "Be professional, constructive, and specific. Format your response clearly."
                ),
                input_variables=["role", "total_score", "max_score", "percentage", "time_taken", "qa_summary"]
            )
            
            chain = recommendation_prompt | llm | StrOutputParser()
            recommendation = chain.invoke({
                "role": st.session_state.role,
                "total_score": total_score,
                "max_score": max_score,
                "percentage": pct,
                "time_taken": f"{int(time_taken // 60)}:{int(time_taken % 60):02d}",
                "qa_summary": qa_summary
            })
            
            # Display recommendation with styling
            if "RECOMMENDED" in recommendation.upper() and "NOT RECOMMENDED" not in recommendation.upper():
                st.success("‚úÖ **Candidate is RECOMMENDED for this role**")
            else:
                st.error("‚ùå **Candidate is NOT RECOMMENDED for this role**")
            
            st.markdown(recommendation)
    
    st.markdown("---")
    
    # Export results button
    if len(st.session_state.qa_history) == 5:
        if st.button("üì• Export Complete Evaluation Report", use_container_width=True):
            out = io.StringIO()
            out.write("="*60 + "\n")
            out.write("CANDIDATE EVALUATION REPORT\n")
            out.write("="*60 + "\n\n")
            out.write(f"Candidate: {st.session_state.candidate.get('name')}\n")
            out.write(f"Role: {st.session_state.role}\n")
            out.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            out.write(f"Total Score: {total_score}/{max_score} ({pct:.1f}%)\n")
            out.write(f"Time Taken: {format_time(time_taken)}\n\n")
            out.write("="*60 + "\n")
            out.write("QUESTION-WISE PERFORMANCE\n")
            out.write("="*60 + "\n\n")
            for i, qa in enumerate(st.session_state.qa_history, start=1):
                out.write(f"Q{i}: {qa['q']}\n")
                out.write(f"Answer: {qa['a']}\n")
                out.write(f"Score: {qa['score']}/20\n")
                out.write(f"Feedback: {qa['feedback']}\n")
                out.write("-"*60 + "\n\n")
            
            filename = f"evaluation_{st.session_state.username}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            st.download_button("üìÑ Download Report", out.getvalue(), file_name=filename, mime="text/plain")
